{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copied this code lmao\n",
    "The code uses numpy so I need to change it to pytorch\n",
    "For that scroll down until you find it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perturbation(model, x1, label_x0, label_k, val_grad):\n",
    "    # Get gradient and function value for class k\n",
    "    gradient_k, fx1 = grad_val(model, x1, label_k)\n",
    "    \n",
    "    # Calculate the difference between gradients\n",
    "    gradient_diff = [g - g0 for g, g0 in zip(gradient_k, val_grad)]\n",
    "    gradient_diff_norm = np.sqrt(np.sum([np.linalg.norm(g)**2 for g in gradient_diff]))\n",
    "    \n",
    "    # Compute the function value difference for class k\n",
    "    f_k = fx1[0, label_k] - fx1[0, label_x0]\n",
    "    \n",
    "    # Compute the weighted gradient ratio\n",
    "    fk_wk = np.linalg.norm(f_k) / (gradient_diff_norm + 1e-3)\n",
    "    \n",
    "    return gradient_diff, fk_wk, f_k\n",
    "\n",
    "def deep_fool_algorithm(model, x0, step=0.01, max_iter=15):\n",
    "    # Get the initial prediction and label of x0\n",
    "    preds = model(x0)[0]\n",
    "    initial_label = np.argmax(preds)\n",
    "    x1 = deepcopy(x0)\n",
    "    current_label = initial_label\n",
    "    loop_iter = 0\n",
    "    perturbations = []\n",
    "\n",
    "    while current_label == initial_label and loop_iter < max_iter:\n",
    "        perturb_direction = [np.zeros_like(x_input) for x_input in x0]\n",
    "        perturb_magnitude = 0\n",
    "        best_l = np.inf\n",
    "        \n",
    "        # Get the gradient and function value for the initial label\n",
    "        val_grad, fx1 = grad_val(model, x1, initial_label)\n",
    "        \n",
    "        # Iterate over all classes except the true label\n",
    "        for label_k in range(10):\n",
    "            if label_k == initial_label:\n",
    "                continue\n",
    "\n",
    "            # Compute perturbation for the current class label\n",
    "            perturb_diff, fk_wk, f_k = compute_perturbation(model, x1, initial_label, label_k, val_grad)\n",
    "            \n",
    "            # Update perturbation if the current perturbation direction is better\n",
    "            if fk_wk < best_l:\n",
    "                perturb_direction, perturb_magnitude = perturb_diff, f_k\n",
    "                best_l = fk_wk\n",
    "        \n",
    "        # Normalize the perturbation\n",
    "        perturb_direction_norm_squared = np.sum([np.linalg.norm(w_l_input)**2 for w_l_input in perturb_direction])\n",
    "        perturb_magnitude_norm = np.linalg.norm(perturb_magnitude)\n",
    "        perturb_constant = perturb_magnitude_norm / (perturb_direction_norm_squared + 1e-3)\n",
    "        \n",
    "        # Scale the perturbation by the calculated constant\n",
    "        perturbation = [perturb_constant * w_l_input for w_l_input in perturb_direction]\n",
    "        perturbations.append(perturbation)\n",
    "\n",
    "        # Update x1 with the new perturbation\n",
    "        x1 = [x1_item + (1 + step) * perturb_item for x1_item, perturb_item in zip(x1, perturbation)]\n",
    "        x1 = np.array(x1)\n",
    "\n",
    "        # Get the new prediction for x1\n",
    "        preds = model(x1)[0]\n",
    "        current_label = np.argmax(preds)\n",
    "        loop_iter += 1\n",
    "\n",
    "    # Sum up all perturbations for the final result\n",
    "    total_perturbation = [np.zeros_like(x_input) for x_input in x0]\n",
    "    for i in range(len(x0)):\n",
    "        for perturb in perturbations:\n",
    "            total_perturbation[i] += perturb[i][0]\n",
    "\n",
    "    return total_perturbation, loop_iter, current_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the pytorch code brah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.cnn import AudioCNN\n",
    "from utils import read_config\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper Functions for DeepFool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jacobian_and_values(model, x, lengths):\n",
    "    \"\"\"\n",
    "    Computes the function values (logits) and the Jacobian (gradients)\n",
    "    for all classes w.r.t the input x.\n",
    "    \n",
    "    This function replaces 'grad_val' from the implementation above and performs the\n",
    "    gradient calculation required.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The target neural network model.\n",
    "        x (torch.Tensor): The input tensor (requires_grad=True).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (f_x, jacobian_dict), where f_x is the logit tensor, and\n",
    "               jacobian_dict maps class index to its gradient tensor.\n",
    "    \"\"\"\n",
    "    # 1. Compute logits\n",
    "    f_x = model(x.unsqueeze(0), lengths) # Add batch dimension\n",
    "    f_x = f_x.squeeze(0)        # Remove batch dimension for 1D logits\n",
    "\n",
    "    # 2. Get the number of classes\n",
    "    num_classes = f_x.numel()\n",
    "    \n",
    "    # 3. Create a dictionary to hold the Jacobian (gradients) for each class\n",
    "    jacobian_dict = {}\n",
    "\n",
    "    # 4. Iterate over each class to compute its gradient w.r.t. x\n",
    "    for k in range(num_classes):\n",
    "        # Zero out the gradients of the model parameters\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Select the logit for class k\n",
    "        f_k = f_x[k]\n",
    "        \n",
    "        # Compute gradient of f_k w.r.t. the input x\n",
    "        # This will populate x.grad if retain_graph=False, but we use torch.autograd.grad\n",
    "        # to explicitly compute the gradient for f_k without side effects\n",
    "        gradient_k = torch.autograd.grad(f_k, x, retain_graph=True)[0]\n",
    "        \n",
    "        jacobian_dict[k] = gradient_k\n",
    "        \n",
    "    return f_x, jacobian_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. DeepFool Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def deep_fool_attack(model, x_original, lengths, step=0.01, max_iter=50, overshoot=0.02, num_classes=10, device='cpu'):\n",
    "    \"\"\"\n",
    "    Performs the DeepFool adversarial attack.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The target neural network model.\n",
    "        x_original (torch.Tensor): The original input data (single sample).\n",
    "        step (float): Step size for moving towards the boundary (redundant with overshoot).\n",
    "        max_iter (int): Maximum number of iterations for the attack.\n",
    "        overshoot (float): Parameter to push the perturbation slightly past the boundary.\n",
    "        num_classes (int): Total number of classes.\n",
    "        device (str): Device (e.g., 'cpu', 'cuda').\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (r_total, loop_iter, final_label), where r_total is the adversarial\n",
    "               perturbation, loop_iter is the number of steps taken, and final_label\n",
    "               is the misclassified label.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure input is on the correct device, requires gradient tracking, and is a single sample\n",
    "    x_i = x_original.clone().detach().to(device)\n",
    "    x_i.requires_grad = True\n",
    "    \n",
    "    # 1. Initial prediction\n",
    "    logits_original = model(x_i.unsqueeze(0), lengths).squeeze(0)\n",
    "    initial_label = logits_original.argmax().item()\n",
    "    current_label = initial_label\n",
    "    \n",
    "    r_total = torch.zeros_like(x_i).to(device)\n",
    "    loop_iter = 0\n",
    "\n",
    "    # Main iterative loop\n",
    "    while current_label == initial_label and loop_iter < max_iter:\n",
    "        \n",
    "        # Compute function values (f_k) and gradients (w_k) for the current x_i\n",
    "        f_x_i, w_k_dict = compute_jacobian_and_values(model, x_i, lengths)\n",
    "        \n",
    "        # Get the gradient for the true class (w_true)\n",
    "        w_true = w_k_dict[initial_label]\n",
    "        f_true = f_x_i[initial_label]\n",
    "        \n",
    "        min_w_norm = float('inf')\n",
    "        min_r_l = None\n",
    "        \n",
    "        # 2. Iterate over all other classes (k != true) to find the nearest boundary\n",
    "        for k in range(num_classes):\n",
    "            if k == initial_label:\n",
    "                continue\n",
    "\n",
    "            w_k = w_k_dict[k]\n",
    "            f_k = f_x_i[k]\n",
    "            \n",
    "            # Compute the vector w_l = w_k - w_true and the value f_l = f_k - f_true\n",
    "            # This defines the separating hyperplane w_l * x + f_l = 0\n",
    "            w_l = w_k - w_true\n",
    "            f_l = f_k - f_true\n",
    "            \n",
    "            # The distance from the current point x_i to the hyperplane is:\n",
    "            # distance = |f_l| / ||w_l||_2\n",
    "            # The perturbation r_l is the projection of x_i onto this hyperplane:\n",
    "            # r_l = - (f_l / ||w_l||_2^2) * w_l\n",
    "            \n",
    "            w_l_norm_sq = w_l.pow(2).sum()\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            if w_l_norm_sq.item() < 1e-8:\n",
    "                continue\n",
    "                \n",
    "            w_norm = w_l_norm_sq.sqrt()\n",
    "            \n",
    "            # Compute the perturbation magnitude and direction\n",
    "            r_l = - (f_l.abs() / w_l_norm_sq) * w_l\n",
    "            \n",
    "            # Check if this boundary is closer\n",
    "            if f_l.abs() / w_norm < min_w_norm:\n",
    "                min_w_norm = f_l.abs() / w_norm\n",
    "                min_r_l = r_l\n",
    "        \n",
    "        # 3. Update the total perturbation and the input\n",
    "        if min_r_l is not None:\n",
    "            r_total += min_r_l\n",
    "            \n",
    "            # The step is applied to the final result, including the overshoot\n",
    "            x_i_new = x_original.clone().detach().to(device) + (1 + overshoot) * r_total\n",
    "            x_i = x_i_new.clone().detach().to(device)\n",
    "            x_i.requires_grad = True # Must re-enable gradient tracking\n",
    "            \n",
    "            # 4. Check the new prediction\n",
    "            with torch.no_grad():\n",
    "                current_label = model(x_i.unsqueeze(0), lengths).squeeze(0).argmax().item()\n",
    "        else:\n",
    "            # Should not happen if the model has multiple classes\n",
    "            break \n",
    "            \n",
    "        loop_iter += 1\n",
    "\n",
    "    return (1 + overshoot) * r_total, loop_iter, current_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the model\n",
    "\n",
    "config = read_config(\"../config/cnn.json\")\n",
    "\n",
    "device = AudioCNN.setDevice()\n",
    "\n",
    "model = AudioCNN.loadModel(path=\"model.pt\", config=config, device=device)\n",
    "\n",
    "# 2. Create a mock input audio sample (e.g., 1-second at 22050 Hz)\n",
    "mock_audio_sample = torch.randn(22050, device=device) \n",
    "lengths = torch.tensor([22050], device=device)\n",
    "\n",
    "# 3. Get initial prediction\n",
    "\n",
    "with torch.no_grad():\n",
    "    initial_logits = model(mock_audio_sample.unsqueeze(0), lengths)  # ← FIX THIS\n",
    "    initial_label = initial_logits.argmax().item()  # ← ADD THIS\n",
    "print(f\"Initial prediction: Class {initial_label}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 4. Run the DeepFool attack\n",
    "print(\"Running DeepFool attack (Max Iterations: 50)...\")\n",
    "r_total, loop_iter, final_label = deep_fool_attack(\n",
    "    model=model, \n",
    "    x_original=mock_audio_sample, \n",
    "    lengths=lengths,\n",
    "    max_iter=50, \n",
    "    overshoot=0.01,\n",
    "    num_classes=10,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# 5. Create adversarial example\n",
    "x_adv = mock_audio_sample + r_total\n",
    "\n",
    "# 6. Verify final prediction\n",
    "with torch.no_grad():\n",
    "    final_logits = model(x_adv.unsqueeze(0), lengths)\n",
    "    final_label_check = final_logits.argmax().item()\n",
    "\n",
    "# 7. Report results\n",
    "print(\"-\" * 30)\n",
    "if initial_label != final_label_check:\n",
    "    print(f\"Attack successful!\")\n",
    "    print(f\"Iterations: {loop_iter}\")\n",
    "    print(f\"Original Label: {initial_label}\")\n",
    "    print(f\"Adversarial Label: {final_label_check}\")\n",
    "    \n",
    "    # Calculate perturbation magnitude (L2 norm)\n",
    "    r_norm = r_total.norm().item()\n",
    "    x_norm = mock_audio_sample.norm().item()\n",
    "    print(f\"Relative Perturbation (L2/L2): {r_norm / x_norm:.4e}\")\n",
    "else:\n",
    "    print(\"Attack failed or max iterations reached without misclassification.\")\n",
    "    print(f\"Iterations: {loop_iter}\")\n",
    "    print(f\"Final Label: {final_label_check} (Same as original)\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
